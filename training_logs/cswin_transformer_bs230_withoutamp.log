(cswin-transformer) ubuntu@haca1002:/nas/thuchk/CSWin-Transformer$ bas
h run_main.sh
`fused_weight_gradient_mlp_cuda` module not found. gradient accumulati
on fusion with weight gradient computation disabled.
Training with a single process on 1 GPUs.
Num: 22.320552
Model CSWin_64_12211_tiny_224 created, param count: 22320552
Data processing configuration for current model + dataset:
        input_size: (3, 224, 224)
        interpolation: bicubic
        mean: (0.485, 0.456, 0.406)
        std: (0.229, 0.224, 0.225)
        crop_pct: 0.9
AMP not enabled. Training in float32.
Scheduled epochs: 1
Train: 0 [   0/5570 (  0%)] Loss:  7.013810 (7.0138) Time:   55/s (  5
5/s) LR: 2.000e-03 Data: 2.975
Train: 0 [  50/5570 (  1%)] Loss:  5.164592 (5.3234) Time:  266/s ( 25
2/s) LR: 2.000e-03 Data: 4.220
Train: 0 [ 100/5570 (  2%)] Loss:  5.215967 (5.2555) Time:  276/s ( 26
3/s) LR: 2.000e-03 Data: 5.234
Train: 0 [ 150/5570 (  3%)] Loss:  5.158434 (5.2265) Time:  275/s ( 26
7/s) LR: 2.000e-03 Data: 6.237
Train: 0 [ 200/5570 (  4%)] Loss:  5.139689 (5.2082) Time:  276/s ( 26
9/s) LR: 2.000e-03 Data: 7.231
Train: 0 [ 250/5570 (  4%)] Loss:  5.146300 (5.1957) Time:  276/s ( 27
0/s) LR: 2.000e-03 Data: 8.227
Train: 0 [ 300/5570 (  5%)] Loss:  5.156160 (5.1858) Time:  276/s ( 27
1/s) LR: 2.000e-03 Data: 9.221
Train: 0 [ 350/5570 (  6%)] Loss:  5.139433 (5.1773) Time:  277/s ( 27
1/s) LR: 2.000e-03 Data: 10.212
Train: 0 [ 400/5570 (  7%)] Loss:  5.128227 (5.1709) Time:  276/s ( 27
2/s) LR: 2.000e-03 Data: 11.212
Train: 0 [ 450/5570 (  8%)] Loss:  5.116321 (5.1652) Time:  276/s ( 27
2/s) LR: 2.000e-03 Data: 12.211
Train: 0 [ 500/5570 (  9%)] Loss:  5.109693 (5.1605) Time:  277/s ( 27
3/s) LR: 2.000e-03 Data: 13.184
Train: 0 [ 550/5570 ( 10%)] Loss:  5.116175 (5.1567) Time:  276/s ( 27
3/s) LR: 2.000e-03 Data: 14.195
Train: 0 [ 600/5570 ( 11%)] Loss:  5.107889 (5.1534) Time:  275/s ( 27
3/s) LR: 2.000e-03 Data: 15.217
Train: 0 [ 650/5570 ( 12%)] Loss:  5.114927 (5.1504) Time:  277/s ( 27
3/s) LR: 2.000e-03 Data: 16.226
Train: 0 [ 700/5570 ( 13%)] Loss:  5.111257 (5.1480) Time:  277/s ( 27
3/s) LR: 2.000e-03 Data: 17.204
Train: 0 [ 750/5570 ( 13%)] Loss:  5.112890 (5.1458) Time:  276/s ( 27
3/s) LR: 2.000e-03 Data: 18.181
Train: 0 [ 800/5570 ( 14%)] Loss:  5.125218 (5.1438) Time:  276/s ( 27
4/s) LR: 2.000e-03 Data: 19.137
Train: 0 [ 850/5570 ( 15%)] Loss:  5.113158 (5.1421) Time:  277/s ( 27
4/s) LR: 2.000e-03 Data: 20.103
Train: 0 [ 900/5570 ( 16%)] Loss:  5.119284 (5.1404) Time:  273/s ( 27
4/s) LR: 2.000e-03 Data: 21.058
Train: 0 [ 950/5570 ( 17%)] Loss:  5.113308 (5.1389) Time:  277/s ( 27
4/s) LR: 2.000e-03 Data: 22.021
Train: 0 [1000/5570 ( 18%)] Loss:  5.107547 (5.1376) Time:  276/s ( 27
4/s) LR: 2.000e-03 Data: 22.990
Train: 0 [1050/5570 ( 19%)] Loss:  5.107852 (5.1363) Time:  276/s ( 27
4/s) LR: 2.000e-03 Data: 23.944
Train: 0 [1100/5570 ( 20%)] Loss:  5.113086 (5.1352) Time:  277/s ( 27
4/s) LR: 2.000e-03 Data: 24.902
Train: 0 [1150/5570 ( 21%)] Loss:  5.093689 (5.1342) Time:  275/s ( 27
4/s) LR: 2.000e-03 Data: 25.856
Train: 0 [1200/5570 ( 22%)] Loss:  5.104660 (5.1332) Time:  275/s ( 27
4/s) LR: 2.000e-03 Data: 26.807
Train: 0 [1250/5570 ( 22%)] Loss:  5.110047 (5.1323) Time:  276/s ( 27
4/s) LR: 2.000e-03 Data: 27.779
Train: 0 [1300/5570 ( 23%)] Loss:  5.115511 (5.1315) Time:  277/s ( 27
4/s) LR: 2.000e-03 Data: 28.819
Train: 0 [1350/5570 ( 24%)] Loss:  5.116637 (5.1307) Time:  277/s ( 27
4/s) LR: 2.000e-03 Data: 29.761
Train: 0 [1400/5570 ( 25%)] Loss:  5.112034 (5.1300) Time:  276/s ( 27
4/s) LR: 2.000e-03 Data: 30.699
Train: 0 [1450/5570 ( 26%)] Loss:  5.121831 (5.1293) Time:  276/s ( 27
4/s) LR: 2.000e-03 Data: 31.637
Train: 0 [1500/5570 ( 27%)] Loss:  5.106283 (5.1287) Time:  277/s ( 27
5/s) LR: 2.000e-03 Data: 32.577
Train: 0 [1550/5570 ( 28%)] Loss:  5.117462 (5.1281) Time:  277/s ( 27
5/s) LR: 2.000e-03 Data: 33.510
Train: 0 [1600/5570 ( 29%)] Loss:  5.117560 (5.1276) Time:  277/s ( 27
5/s) LR: 2.000e-03 Data: 34.450
Train: 0 [1650/5570 ( 30%)] Loss:  5.103038 (5.1270) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 35.376
Train: 0 [1700/5570 ( 31%)] Loss:  5.107547 (5.1266) Time:  278/s ( 27
5/s) LR: 2.000e-03 Data: 36.329
Train: 0 [1750/5570 ( 31%)] Loss:  5.115375 (5.1261) Time:  277/s ( 27
5/s) LR: 2.000e-03 Data: 37.271
Train: 0 [1800/5570 ( 32%)] Loss:  5.107375 (5.1256) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 38.243
Train: 0 [1850/5570 ( 33%)] Loss:  5.109700 (5.1252) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 39.243
Train: 0 [1900/5570 ( 34%)] Loss:  5.112846 (5.1248) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 40.230
Train: 0 [1950/5570 ( 35%)] Loss:  5.106573 (5.1243) Time:  274/s ( 27
5/s) LR: 2.000e-03 Data: 41.232
Train: 0 [2000/5570 ( 36%)] Loss:  5.106136 (5.1240) Time:  274/s ( 27
5/s) LR: 2.000e-03 Data: 42.249
Train: 0 [2050/5570 ( 37%)] Loss:  5.123211 (5.1236) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 43.259
Train: 0 [2100/5570 ( 38%)] Loss:  5.106606 (5.1233) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 44.267
Train: 0 [2150/5570 ( 39%)] Loss:  5.107881 (5.1229) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 45.284
Train: 0 [2200/5570 ( 40%)] Loss:  5.110444 (5.1227) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 46.301
Train: 0 [2250/5570 ( 40%)] Loss:  5.112391 (5.1223) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 47.312
Train: 0 [2300/5570 ( 41%)] Loss:  5.106087 (5.1221) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 48.329
Train: 0 [2350/5570 ( 42%)] Loss:  5.104521 (5.1218) Time:  275/s ( 27
5/s) LR: 2.000e-03 Data: 49.343
Train: 0 [2400/5570 ( 43%)] Loss:  5.108572 (5.1215) Time:  277/s ( 27
5/s) LR: 2.000e-03 Data: 50.362
Train: 0 [2450/5570 ( 44%)] Loss:  5.110172 (5.1212) Time:  275/s ( 27
5/s) LR: 2.000e-03 Data: 51.381
Train: 0 [2500/5570 ( 45%)] Loss:  5.109532 (5.1210) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 52.396
Train: 0 [2550/5570 ( 46%)] Loss:  5.108624 (5.1207) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 53.416
Train: 0 [2600/5570 ( 47%)] Loss:  5.107882 (5.1205) Time:  277/s ( 27
5/s) LR: 2.000e-03 Data: 54.552
Train: 0 [2650/5570 ( 48%)] Loss:  5.111282 (5.1203) Time:  277/s ( 27
5/s) LR: 2.000e-03 Data: 55.572
Train: 0 [2700/5570 ( 48%)] Loss:  5.104380 (5.1201) Time:  269/s ( 27
5/s) LR: 2.000e-03 Data: 56.580
Train: 0 [2750/5570 ( 49%)] Loss:  5.097222 (5.1199) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 57.584
Train: 0 [2800/5570 ( 50%)] Loss:  5.107755 (5.1197) Time:  277/s ( 27
5/s) LR: 2.000e-03 Data: 58.589
Train: 0 [2850/5570 ( 51%)] Loss:  5.114894 (5.1195) Time:  277/s ( 27
5/s) LR: 2.000e-03 Data: 59.602
Train: 0 [2900/5570 ( 52%)] Loss:  5.108692 (5.1193) Time:  277/s ( 27
5/s) LR: 2.000e-03 Data: 60.630
Train: 0 [2950/5570 ( 53%)] Loss:  5.116796 (5.1191) Time:  276/s ( 27
5/s) LR: 2.000e-03 Data: 61.657
Train: 0 [3000/5570 ( 54%)] Loss:  5.110936 (5.1189) Time:  275/s ( 27
5/s) LR: 2.000e-03 Data: 62.681
Train: 0 [3050/5570 ( 55%)] Loss:  5.109757 (5.1188) Time:  241/s ( 27
5/s) LR: 2.000e-03 Data: 63.695
(cswin-transformer) ubuntu@haca1002:/nas/thuchk/CSWin-Transformer$ tmu
x capture-pane -pS -300 > cswin_transformer_bs230_withoutamp.log

